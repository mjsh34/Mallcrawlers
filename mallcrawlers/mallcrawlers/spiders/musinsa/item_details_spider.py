from mallcrawlers.items import MusinsaItemDetailsItem, MusinsaItemReviewItem

import scrapy
from scrapy import Request
from scrapy.utils.defer import maybe_deferred_to_future
from twisted.internet.defer import DeferredList
from urllib.parse import urlparse
from urllib.parse import parse_qs

import csv
import json
import re
import os
import os.path as osp
from pathlib import Path


class MusinsaItemDetailsSpider(scrapy.Spider):
    name = "musinsa__item_details"

    @classmethod
    def update_settings(clazz, settings):
        super().update_settings(settings)
        settings.set("USER_AGENT", settings['USER_AGENT_CHROME_WIN10'], priority='spider')

    def __init__(self, csv_path_or_dir, review_order="최신순", review_page_limit=-1):
        csv_paths = []
        if osp.isdir(csv_path_or_dir):
            csv_paths.extend(Path(csv_path_or_dir).iterdir())
        elif osp.isfile(csv_path_or_dir):
            csv_paths.append(Path(csv_path_or_dir))
        else:
            raise ValueError("csv_path_or_dir arg must be path to a csv file generated by "
                    "a 'musinsa__items' spider, or a directory containing them")
        self.logger.info("Found %d musinsa items csvs", len(csv_paths))

        # start urls
        self.item_list = []
        for csv_path in csv_paths:
            with open(csv_path, encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for irow, row in enumerate(reader):
                    d = {**row}
                    url = row['item_url'].strip()
                    if url.startswith("//"):
                        url = "https:" + url
                    elif not url.startswith("http"):
                        self.logger.warning("Invalid url found: '%s' at row %d of '%s'", url, irow+1, csv_path)
                        continue
                    d['url'] = url
                    d['goods_no'] = row['data_no']
                    self.item_list.append(d)
        #self.item_list = [{'url': "https://www.musinsa.com/app/goods/996177", 'goods_no': '996177'}]
        self.items_count = len(self.item_list)
        self.review_order = review_order
        self.review_page_limit = int(review_page_limit)
        self.__delay_http_codes_default = {403: 100}
        super().__init__()

    def start_requests(self):
        start_req_objs = []
        for iitem, item in enumerate(self.item_list):
            start_req_objs.append(Request(item['url'], callback=self.parse,
                meta={**item, 'delay_http_codes': self.__delay_http_codes_default,
                    'musinsa__item_details_page': True,
                    'musinsa__item_crawl_index': iitem}))
        return start_req_objs

    async def parse(self, response):
        crawl_idx = response.meta.get('musinsa__item_crawl_index', -2)
        self.logger.info("Parsing Musinsa item details %d/%d [%.02f%%] (%s)",
                crawl_idx+1, self.items_count, 100*(crawl_idx/self.items_count),
                response.meta.get('goods_no', "?"))
        goods_no = response.meta['goods_no']
        # stateAll, stateAllV2
        stateall = None
        stateallv2 = None
        for script in response.css("script::text").getall():
            for line in script.split('\n'):
                line = line.strip()
                if line.startswith("stateAll = {"):
                    stateall = json.loads(line[line.index("{")-1:line.rindex("}")+1])
                elif line.startswith("stateAllV2 = {"):
                    stateallv2 = json.loads(line[line.index("{")-1:line.rindex("}")+1])
        # essential & actual-size
        deferred_reqs = [
            Request("https://goods-detail.musinsa.com/goods/{}/essential".format(goods_no),
                meta={'delay_http_codes': self.__delay_http_codes_default, 'musinsa__item_details_essential': True}),
            Request("https://goods-detail.musinsa.com/goods/{}/actual-size".format(goods_no),
                meta={'delay_http_codes': self.__delay_http_codes_default, 'musinsa__item_details_actualsize': True})
        ]
        additional_response = await maybe_deferred_to_future(
            DeferredList([self.crawler.engine.download(r) for r in deferred_reqs]))
        essential = json.loads(additional_response[0][1].text)
        actualsize = json.loads(additional_response[1][1].text)
        yield MusinsaItemDetailsItem({
                'goods_no': goods_no,
                'stateall': stateall,
                'stateallv2': stateallv2,
                'essential': essential,
                'actualsize': actualsize,
            })
        # reviews
        for review_type in ["스타일 후기", "상품 후기", "일반 후기"]:
            yield response.follow(self.__get_reviews_url(response.meta, page=1, review_type=review_type),
                    callback=self.parse_reviews,
                    meta={'review_page': 1, 'goods_no': goods_no,
                        'review_type': review_type, 'musinsa__item_details_review': True,
                        'delay_http_codes': self.__delay_http_codes_default})

    def parse_reviews(self, response):
        goods_no = response.meta['goods_no']
        review_type = response.meta['review_type']
        # Get page info
        review_page = None
        if 'review_page' in response.meta:
            review_page = response.meta['review_page']
        else:
            try:
                qs = parse_qs(urlparse(response.url).query)
                review_page = int(qs.get('page', [None])[0])
            except:
                self.logger.error("Failed to fetch review page (%s).", response.url)

        review_elms = response.css(".review-list-wrap .review-list")
        review_lastpage = 1 if len(review_elms) == 0 else None
        if len(review_elms) > 0 and review_page is not None:
            review_lastpage = None
            if 'review_page_last' in response.meta:
                review_lastpage = response.meta['review_page_last']
            else:
                review_npages_txt = response.css(".nslist_bottom .box_page_msg::text").get()
                if review_npages_txt:
                    m = re.search(r"^(\d+)\s페이지\s중\s\d+\s페이지$", review_npages_txt.strip())
                    if m:
                        review_lastpage = int(m.group(1))
        self.logger.info("%d '%s' reviews found for item %s on page %d/%d.",
                len(review_elms), review_type, goods_no, review_page or -1, review_lastpage or -1)

        # Parse reviews in current page
        for rev in review_elms:
            d = {
                'review_type': review_type,
                'goods_no': goods_no,
            }
            d['reviewer_name'] = rev.css(".review-profile__name::text").get()
            d['review_date'] = rev.css(".review-profile__date::text").get()
            d['reviewer_profile'] = rev.css(".review-profile__body_information::text").get()
            revi = rev.css(".review-goods-information")
            d['item_url'] = rev.css(".review-goods-information__item a[href]::attr(href)").get()
            d['item_name'] = rev.css(".review-goods-information__name::text").get()
            d['item_size'] = rev.css(".review-goods-information__option::text").get().strip()
            d['rating_active'] = rev.css(".review-list__rating").get()
            revc = rev.css(".review-contents")
            d['review_list'] = []
            for rli in revc.css(".review-evaluation--type3__list .review-evaluation--type3__item"):
                d['review_list'].append(rli.css("::text").getall())
            d['review_text'] = revc.css(".review-contents__text::text").get()
            d['review_photos'] = []
            for rpi in revc.css(".review-content-photo .review-content-photo__list .review-content-photo__item img[src]"):
                d['review_photos'].append(rpi.attrib['src'])
            d['comments'] = rev.css(".recomment_text .cmt_summary::text").getall()
            d['review_page'] = review_page
            d['review_page_last'] = review_lastpage
            yield MusinsaItemReviewItem(d)

        # Get page info
        if review_page is not None and review_lastpage is not None:
            last_review_page_to_scrape = min(self.review_page_limit, review_lastpage) \
                    if self.review_page_limit >= 0 else review_lastpage
            if review_page < last_review_page_to_scrape:
                review_page_next = review_page + 1
                self.logger.info("'%s' review next page %d --> %d /%d (%d)",
                        review_type, review_page, review_page_next, last_review_page_to_scrape, review_lastpage)
                yield response.follow(self.__get_reviews_url(response.meta, page=review_page_next),
                        callback=self.parse_reviews,
                        meta={'review_page': review_page_next, 'review_type': review_type,
                            'musinsa__item_details_review': True, 'goods_no': goods_no,
                            'delay_http_codes': self.__delay_http_codes_default},
                        errback=self.__errback_item_details_review)

    def __errback_item_details_review(self, failure):
        self.logger.error("Failed to fetch item details review (%s): %s", failure.request.url, repr(failure))

    def __get_reviews_url(self, meta, page=1, review_type="스타일 후기", order=None):
        goods_no = meta['goods_no']
        review_type_url = {"스타일 후기": "style", "상품 후기": "photo", "일반 후기": "goods"}[review_type]
        review_order_url = {
            "유용한 순": "up_cnt_desc",
            "최신순": "new",
            "댓글순": "comment_cnt_desc",
            "높은 평점 순": "goods_est_desc",
            "낮은 평점 순": "goods_est_asc",
        }[order or self.review_order]
        return (f"https://goods.musinsa.com/api/goods/v2/review/{review_type_url}/list"
            + f"?similarNo={goods_no}&sort={review_order_url}&selectedSimilarNo={goods_no}"
            + f"&page={page}&goodsNo={goods_no}")

